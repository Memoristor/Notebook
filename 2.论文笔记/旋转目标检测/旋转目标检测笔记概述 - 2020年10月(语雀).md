# 旋转目标检测

**本次旋转目标检测的笔记是针对即将进行的火箭军AI大赛科目四做的相关调研，总结了截止2020年10月以来，在遥感图像的小目标、密集、任意方向下的旋转目标检测方法，以及当前已有方法下仍旧存在的问题**。仅作入门，如有纰误，敬请指正。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201010184443353.png" alt="img" width=350 />
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201010184451186.png" alt="img" width=350 />
</div>
<br>

## 一、笔记介绍

* 目前目标检测的工作集中在三个方面：**水平区域目标检测（Horizontal region object detection），任意方向目标检测（Arbitrary-oriented object detection），方位信息分类（Classification for orientation information）**。**水平区域目标检测**即为经典的目标检测方法，旨在利用常规水平框检测图像的目标，如`FasterRCNN`，`YOLO`，`SSD`是常见的**Anchor-based**的方法，而`CenterNet`，`CornerNet`则是**Anchor-free**的方法；**任意方向目标检测**则是希望对任意方向上的目标能够用一个四边形的框更准确的表示物体的范围，主流的方法可分为三种：基于旋转候选框的五参数法、基于候选框顶点偏移的八参数法和基于极坐标角度回归的五参数法。常见Baseline有`ICN`，`	ROI-Transformer`，`SCRDet`，`R3Det`等方法。最后一种**方位信息分类**的方法则是用于诸如人脸方向检测等，不在该笔记的讨论之内。
* 遥感旋转检测研究比文字晚一些，目前也有方法是从文字检测中迁移过来，但个人认为如果从旋转框角度来做的话，遥感可能会更难一些。毕竟文字是单类别，很多方法都是提出有针对性的tricks，而遥感更需要考虑不同物体的类别，直接放到遥感就不适用。遥感的旋转目标检测需要考虑更general的思路，其难点主要包括小目标 （small objects）、密集 （cluttered arrangement）、方向任意（arbitrary orientations）。

* 根据已有的旋转目标检测的代表性方法（不完全统计，更多统计的点击**第5.1节的链接[3]**），按照是否基于Anchor，单/两阶段，参数类型进行分类：

  |       Model        |   Backbone   |                          Paper Link                          | Stage  |    Anchor    |                          Parameters                          |
  | :----------------: | :----------: | :----------------------------------------------------------: | :----: | :----------: | :----------------------------------------------------------: |
  |  R<sup>3</sup>Det  |  ResNet152   |        [HRSC 2016](https://arxiv.org/abs/1908.05612)         | Single | Anchor-based |                   5-Params: Center+(w,h)+θ                   |
  |  R<sup>2</sup>CNN  |  ResNet101   |        [CVPR 2017](https://arxiv.org/abs/1706.09579)         |  Two   | Anchor-based | 5-Params: (x<sub>1</sub>,y<sub>1</sub>,x<sub>2</sub>,y<sub>2</sub>,h) |
  |       SCRDet       |  ResNet101   | [ICCV 2019](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf) |  Two   | Anchor-based |                   5-Params: Center+(w,h)+θ                   |
  |   Gliding Vertex   |  ResNet101   |  [TPAMI 2019](https://ieeexplore.ieee.org/document/9001201)  | Single | Anchor-free  | 8-Params: Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) |
  |      P-RSDet       |  ResNet101   |     [arXiv:2001.02988](https://arxiv.org/abs/2001.02988)     | Single | Anchor-free  |    5-Params: Center+*ρ*+(*θ<sub>1</sub>*,*θ<sub>2</sub>*)    |
  | O<sup>2</sup>-DNet | Hourglass104 |     [arXiv:1912.10694](https://arxiv.org/abs/1912.10694)     | Single | Anchor-free  | 8-Params: 2x(Δx<sub>1</sub>,Δx<sub>2</sub>,Δy<sub>1</sub>,Δy<sub>2</sub>) |
  |     BBAVector      |  ResNet101   |        [WACV 2020](https://arxiv.org/abs/2008.07043)         | Single | Anchor-free  |      12-Params: Center+(**t**,**r**,**l**,**b**)+(w,h)       |

<br>

## 二、预测参数的定义

本节主要介绍旋转目标检测中具有代表性和创新性的旋转框定义方法，当然并不局限于这几种，但是殊途同归，很多不同的表示方式都基于相似的思想并存在相同的问题。

### 2.1 Center+(w,h)+θ 五参数法

Center+(w,h)+θ 五参数法经典的方法有`RRPN`，`ROI-Transformer`，`R2CNN`，`R3Det`等，目前有两种主流表示方法：OpenCV法（a）和长边表示法（b）。

* OpenCV表示法（a），回归角度为-90到0度：
  * 沿**水平方向**定义参考线，位于该参考线上垂直坐标最小的顶点作为参考原点
  * **逆时针旋转**参考线，参考线接触的第一个矩形边定义为**宽度w**，而与该边垂直的另一条边定义为**高度h**
  * 旋转矩形框的中心点坐标为（x，y），旋转角度为 θ， θ∈[-90, 0]
* 长边表示法（b），回归角度为-90度到90度：
  * 沿**水平方向**定义参考线，选取矩形框的一个**最长边h**，并以该边水平方向最小的顶点作为参考原点
  * 角度 θ 为**长边h**与水平方向的**x轴**所形成的夹角，因此角度的范围是 θ∈[-90, 90]

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201010084120280.png" alt="img" width=450 />
</div>



虽然这两种表示方式在实际遥感和文字的旋转目标检测应用中取得了比较出色的成果，但是由于大长宽比的目标、训练的损失对于角度的变化和边交换是非常敏感的，因此这两种表示方法都存在**回归难度大**、**损失不连续**、**水平框预测差**，**度量单位（量纲）不一致**的问题。

### 2.2 Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) 八参数法

Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) 八参数法是一种更为巧妙的四边形定义法，相比五参数法需要在训练前对四边形框的四个点按照一定规则进行排序。首先了解一下为什么要排序：如果一个四边形的ground-truth是（x1,y1,x2,y2,x3,y3,x4,y4）并且所有的ground-truth并不是按一定规则顺序标注的，那么检测器有可能给出的预测结果是（x2,y2,x3,y3,x4,y4,x1,y1）。其实这两个是框是完全重合的，但是网络训练算损失的时候并不知道，它会按对应位置计算损失，此时的损失值并不为0甚至很大。而Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>)八参数法则考虑到了顺序标签点（sequential label points）的问题。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201011232343294.png" alt="img" width=500 />
</div>

基于上述问题，[Gliding Vertex](https://ieeexplore.ieee.org/document/9001201)通过改变框的表示方式避免了排序的麻烦：

> By limiting the offset on the corresponding side of horizontal bounding box, we may facilitate offset learning and also avoid the confusion for sequential label points in directly regressing the four vertices of oriented objects. 

先检测水平框，这个是没有序列问题的，然后**学习水平框四个角点的偏移量**来达到四边形检测的目的，其实这里的（偏移量，对应的水平框的点）配对就有排序的意思了。相比较而言，[RSDet](https://arxiv.org/pdf/1911.08299.pdf)则是对角点进行了排序（对ground-truth的排序并不耗时，对检测器影响几乎忽略不计），并给出了一种角点的排序算法：其主要步骤是先确定最左边的点（如果水平矩形的话有两个点满足取上面的那个）。然后通过[叉乘（向量积）](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%90%91%E9%87%8F%E7%A7%AF/4601007%3Ffr%3Daladdin)找到对角点，也就是第三个点。最后利用这两个点构成的向量以及叉乘方法，根据顺序的要求（逆时针或者顺时针）找点其他两个点。

八参数法一定程度上解决了五参数法中度量单位不一致的弊端，而且相比于对矩形框进行旋转并预测的五参数法，八参数法能够表征更为复杂的四边形框，对具有仿射变换性质的图像（如文字检测）中，能直接避免五参数法后续需要对检测框进行微调的问题。但是依然存在**回归难度大**、**损失不连续**、**水平框预测差**的问题。

### 2.3 Center+*ρ*+(*θ<sub>1</sub>*,*θ<sub>2</sub>*) 极坐标五参数法

上述五参数法和八参数法都有一个共同的特征：**基于笛卡尔坐标系**对物体对象进行建模，这种直角坐标系在呈现水平边界框方面具有优势，但是在旋转目标检测上却显得尤为复杂；极坐标在旋转和方向相关的问题上具有优势，极坐标的合理使用可以简化对象建模并降低模型的复杂性。[P-RSDet](https://arxiv.org/pdf/2001.02988.pdf)极坐标下的目标检测器, 提出了一种新颖的以极坐标为模型的**Anchor-free**检测器来检测遥感图像的对象，这使得定向输出形式的获取与水平形式的获取一样简单。这一称为极坐标遥感物体检测器（P-RSDet）的模型以每个物体的中心点为极点，以水平正方向为极轴来建立极坐标系。可以将一个物体的检测视为水平和定向包围盒的一个极半径和两个极角的预测。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012095919099.png" alt="img" width=400 />
</div>

* 在极坐标中，一个点可以用（ρ;θ）表示，其中r是极半径，q是极角。 当以中心点为极坐标的极点时，定向边界框可以用（x; y;ρ）表示，如（d）所示。对于一个边界框，根据原始注释（x1，y1），（x2，y2），（x3，y3），（x4，y4），令在极坐标系下的极点为 Center（x<sub>p</sub>，y<sub>p</sub>）。 由于人工标注的误差，四个角与极点之间的距离不一定相等，因此将四个半径的平均值作为极半径的目标回归值。P-RSDet的其余参数定义方法为：
  * 以每个物体的中心点为极点，以水平正方向为极轴来建立极坐标系，并将极角以弧度表示
  * 在这个坐标系中，四个角可以依次表示为（ρ1，θ1），（ρ2，θ2），（ρ3，θ3），（ρ4，θ4）
  * 根据矩形的属性，我们可以得到以下内容关系：ρ1 = ρ2 = ρ3 = ρ4，θ3 = θ1 + π; θ4 = θ2 + π
  * 因此，令ρ=ρ1=ρ2=ρ3=ρ4，仅需要五个变量：Center，ρ，θ1和θ2来表示极坐标中对象的边界框

### 2.4 Center+(**t**,**r**,**l**,**b**)+(w,h) 边缘感知向量

为了得到带方向的包围框，一个很自然的想法就是回归出宽，高，和角度θ，我们把这个baseline称为Center+wh+θ的五参数法，如图（a）。这个方法有几个缺点：（1）小的角度的变化对于损失来说可能是微不足道的，但是对于IOU却会产生较大的影响；（2）Oriented bounding box 中的w，h是在各自独立的旋转坐标系统中定义的，角度θ是相对于y轴的，这样，网络很难去联合训练所有的物体；（3）w,h和θ的度量单位不一致，和IoU之间表现出的是不同的关系。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012111648393.png" alt="img" width=400 />
</div>

所以，[BBAVector](https://arxiv.org/abs/2008.07043)提出了一种描述OBB的方法，叫做box boundary-aware vectors，包括4个向量，**t，r，b，l**，共12个预测参数。在此设计中，这4个向量分布在**笛卡尔坐标系的4个象限**中，所有的旋转物体都共用一个坐标系，这样可以高效的利用共同的信息，并提升模型的泛化能力。模型中设计了4个向量，而不是2个，为的是当某些局部特征不是很明确的时候可以得到更多的边界交互信息。但是在实际做的过程中，在一些极端情况下，向量和象限的边界非常的靠近，如图（c），这样区分向量的类型就比较困难，即**水平框预测差**，这是旋转框预测的常见问题，参见第三节。

## 三、存在的问题及解决方案

### 3.1 回归边界问题

回归边界问题体现在，由角度的周期性（**PoA**）和边的可交换性（**EoE**）导致**理想的回归路线的损失很大**（也就是在实际训练过程中，模型走了弯路）。YangXue在[SCRDet ICCV2019](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf)以及[CSL ECCV 2020](http://arxiv.org/abs/2003.05597)中均有提出：

>loss of this situation is very large due to the periodicity of angular (PoA) and exchangeability of edges (EoE)  

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012152356555.png" alt="img" width=800 />
</div>

* 基于90°回归的方法，即OpenCV表示法，如上图（a）所示。蓝色框、红色框和绿色框分别是参考候选框，预测框和真实框，**特别注意这几个框的长h和宽w所在的位置！！**最理想的角度回归路线是由参考候选框（蓝色）逆时针旋转得到预测框（红色），预测框（红色）和真实框（绿色）便可以基本重合。但是模型并不会将hw进行交换（矩形框存在边的可交换性**EoE**），导致理想的回归的损失非常大。实际训练过程中，模型会迫使参考候选框顺时针旋转到灰色框的等效角度（角度的周期性**PoA**）并缩放长宽，从而加大了回归的难度。（个人理解：角度的周期性PoA应该描述为**角度的等效性**更加合适）
* 基于180°回归的方法，即长边表示法，如上图（b）所示，该方法避免了矩形框的边的可交换性EoE的问题，但是类似地，该方法也存在由**PoA**引起的损失剧增的问题，即该模型最终将参考候选框顺时针旋转一个大角度，以获得最终的预测边界框。**（注意这里图有错误：蓝色候选框，绿色真实框，红色预测框的角度应该分别是π/2，5π/8，3π/8**）
* 基于角点偏移的方法，如图（c）所示。 通过进一步分析：由于角点的提前排序，八参数回归方法中仍然存在边界不连续性问题。考虑边界情况下八参数回归的情况，理想的从蓝色参考框到绿色真实框的回归过程应如虚线箭头所示：（<span style="color:blue;">a</span>-><span style="color:green;">b</span>）（<span style="color:blue;">b</span>-><span style="color:green;">c</span>）（<span style="color:blue;">c</span>-><span style="color:green;">d</span>）（<span style="color:blue;">d</span>-><span style="color:green;">a</span>）。但是实际回归过程中，模型不可能交换点，比如把<span style="color:green;">b</span>变为<span style="color:blue;">a</span>，因此实际的回归为：（<span style="color:blue;">a</span>-><span style="color:green;">a</span>）（<span style="color:blue;">b</span>-><span style="color:green;">b</span>）（<span style="color:blue;">c</span>-><span style="color:green;">c</span>）（<span style="color:blue;">d</span>-><span style="color:green;">d</span>）。 相比之下，虽然蓝色到红色边界框的实际和理想回归是一致的，但是这种实际的回归过程却会导致损失变大。

综上YangXue在[CSL](https://arxiv.org/abs/2003.05597)中得出这样的结论：**理想的预测结果超出了我们所定义的范围，导致出现边界问题，即产生一个较大的损失值**。

>We argue that the root cause of boundary problems based on regression methods is that the ideal predictions are beyond the defined range. 

针对上述问题，YangXue提出：是否可以将角度回归的方式转换成分类的形式？因为分类的结果是有限的，则不会出现超出定义范围外的情况。一种最简单的角度分类方式就是将整个定义的角度范围进行类别的划分，比如一度归为一类。称之为：**环形平滑标签（Circular Smooth Label , CSL）**，如下图所示，其中g(x)是窗口函数，r是窗口函数的半径，θ表示当前边界框的角度。利用脉冲函数、矩形函数、三角函数、高斯函数构建不同类型的环形平滑标签CSL。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012194803517.png" alt="img" width=400 />
</div>

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012190151939.png" alt="img" width=500 />
</div>

自然，将一个连续到离散的问题，在这个转换的过程中是会有精度的损失的。比如在一度一类（w=1）的情况下，我们是无法预测出0.5度这种结果的，文中提到：**假如，有两个相同的长宽比1:9的同中心的矩形，角度相差0.5和0.25，则他们之间的IoU只下降了0.05和0.02。这对于最后评测其实影响非常小，毕竟也不是所有目标都有这么大的长宽比**，计算如下：

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012191442474.png" alt="img" width=500 />
</div>

由于窗口函数的设置，使得模型可以衡量预测标签和真实标签之间的角度距离，即在一定范围内越靠近真实值的预测值的损失值越小。而且通过引入周期性解决了角度周期性的问题，即**使得89和-90两个度数变成是近邻的**。需要注意的是，当窗口函数是脉冲函数或者窗口函数的窗口半径很小时，Circular Smooth Label 等价于One-hot label。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012192003597.png" alt="img" width=600 />
</div>

根据上表可以的出以下几个结论：

- 由于EoE问题的存在， 90°-CSL-baesd方法总体不如180°-CSL-baesd方法；
- 基于高斯窗口函数的方法效果最好，而基于脉冲窗口函数（One-hot label）的效果最差，几乎预测不出角度值；

**其实基于分类预测角度的想法很早在人脸方面有一些工作。CSL证明了该方法在基于180°回归的有效性，但是确实损失了一定的精度，导致IoU性能指标的下降；而且如果角度类别太多，会导致预测分类的输出过分厚重；此外，该方法仅适用于基于角度回归的五参数法，对基于角点偏移的八参数回归则不适用**

### 3.2 回归损失不连续

在3.1中所描述到的边界问题，[RSDet CVPR2019](https://arxiv.org/abs/1911.08299)的作者也发现了这个问题，不同于CSL将连续的回归问题转化为离散的分类问题，他们尝试从**回归损失不连续**的角度直面这个弊端（Loss Discontinuity）。如3.1节中**基于90°回归**的方法所示，在该五参数回归方法中，假设真实框为绿色信息为[ 0 , 0 , 25 , 100 , -10 °] ，预选框为蓝色信息为[0 , 0 , 100 , 25 , -90 °]，经过网络的回归后对预选框进行逆时针旋转生成的预测框为红色[ 0 , 0 , 100 , 25 , -100 °]，但是这个 -100° 角度并不在范围之内，虽然这个过程在物理上是连续的，但是对于损失计算将会很大。对于网络来说，网络会将预选框进行顺时针旋转，也就是灰色的过程，这是一个相对于逆时针变化更复杂的回归，增加了训练难度。将这个现象反映为loss的话，如图（a）所示可以明显观察到**回归损失不连续仅在边界情况下发生！**这也是导致水平框预测差的一大原因。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012185211633.png" alt="img" width=500 />
</div>

为改进这个问题，RSDet提出将原来的不连续的L1-loss改为连续的l<sub>mr</sub>：

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012204110417.png" alt="img" width=500 />
</div>

l<sub>cp</sub>是中心点损失；l<sub>mr</sub>的第一项是L1-loss。第二项是**通过消除角度周期性**以及**高度和宽度的交换性**校正损失使其连续。当其未达到角度参数的范围边界时，该校正项大于L1-loss。当L1-loss突然发生不连续时，这种校正使损失变得正常。换句话说，这种校正可以看作是对突变位置的L1-loss进行对称。最后，损失为最小的L1-loss和校正损失。l<sub>mr</sub>曲线是连续的，如上图b所示。

类比这种思想，基于Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) 八参数法也可以有相似的连续损失表现形式，即可以通过修改损失的方式解决3.1节图（c）中模型无法正确交换顶点的问题。作者分为了三个部分来讨论：

* 将预选框的四个顶点顺时针移动一定位置
* 保持预选框顶点的顺序不变
* 将预选框的四个顶点逆时针移动一个位置

取这三种情况损失的最小值，因此得到的连续损失l<sub>mr</sub>：

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012204659917.png" alt="img" width=450 />
</div>

作者在RetinaNet上对五参数回归法和八参数回归法进行试验，如下图所示，从损失的收敛曲线可以得到两个结论：

* 连续损失l<sub>mr</sub>在五参数法和八参数法下很少出现损失忽然剧增的问题
* 连续损失l<sub>mr</sub>的八参数法收敛趋势比五参数法更为平滑，说明八参数法比五参数法更容易回归（这点有点存疑，两张图的Scale和Epoch不一致啊哥们儿）

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012205832333.png" alt="img" width=500 />
</div>

在DOTA基准上进行l<sub>mr</sub>和预定义的八参数回归的消融实验，RetinaNet-H作为baseline的对比结果：

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012211408409.png" alt="img" width=500 />
</div>
### 3.3 水平框预测差

在3.1，3.2中叙述到的模型在回归边界的回归难度大、损失不连续都是水平框预测差的主要原因之一，因此很多已有的旋转目标检测模型都存在**水平框预测能力表现差**的问题，如图（a）所示。

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012154018237.png" alt="img" width=700 />
</div>

在目前众多已有的遥感旋转目标检测方法中，解决这个问题的主要方法就是**通过预测物体的水平指标判断其是否水平的，如果是，则只需要用水平框表示即可**。这种<span style="color:red;">治标不治本</span>的方法虽然离谱，而且牺牲了一定的预测精度（通过水平框来近似）来逃避了边界预测不好的问题，但是确实也是有一定效果的：更加稳定，还能涨点。 例如这个问题在Gliding Vertex提出的定义方法中表现得更加明显，作者认为水平框的样本很少是导致边界情况预测不好的原因（作者并没详细说明）。为了解决这个问题，作者提出了一个叫obliquity factor的预测值，即**旋转框和其对应水平框的面积比值**，来控制当前所要预测目标是否可以直接采用（x,y,w,h）来表示就行了，下面就是一个示意图：

> Indeed, it is reasonable to represent nearly horizontal objects with horizontal bounding boxes. However, oriented detections are required to accurately describe oriented objects. 

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012155350922.png" alt="img" width=450 />
</div>
在3.1节和3.2节中提到的使用**环形平滑标签CSL**和**修改损失使其连续**的方法在很大程度上能够改善水平框预测困难的问题。当然，解决这个问题的方法并不唯一。在Anchor-based，two-stage的[SCRDet](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf)中也有提到使用IoU-Smooth L1 loss，即在传统的smooth L1 损失函数中引入了IoU常数因子。**在边界情况下，新的损失函数近似等于0，消除了损失的突增**。新的回归损失可分为两部分，smooth L1回归损失函数取单位向量确定梯度传播的方向，而IoU表示梯度的大小，这样loss函数就变得连续，如下所示：

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201013100604568.png" alt="img" width=450 />
</div>

这里N是候选框的个数，L<sub>cls</sub>是分类损失（softmax交叉熵），L<sub>reg</sub>是参数回归损失（smooth-l1），L<sub>latt</sub>是文中使用到的注意力损失（逐像素的softmax交叉熵），但是作者在知乎上也提到：

>IoU-Smooth L1 Loss是在RetinaNet旋转检测代码上进行检验的，发现效果出奇的好，从62.25涨到68.65，但也发现稍微改动配置文件就NAN，难搞

出现NAN的原因主要就是用了对数函数，即log(IoU)，解决的办法有两个：log(IoU+δ)，这里δ是一个比较小的数，这个方法比较粗暴；另一个方法就是将log(IoU)换成1-exp(1-IoU)，也可以达到类似的效果。

### 3.4 度量单位（量纲）不一致

[RSDet](https://arxiv.org/abs/1911.08299)除了通过让loss变得连续让模型能够有更好的收敛效果，作者还发现**五参数回归方法**的**不同度量单位（量纲）**会使回归不一致。下图研究了所有参数与IoU之间的关系。IoU和宽度（高度）之间的关系是线性函数和反比例函数的组合。中心点与IoU之间的关系是对称的线性函数，如b所示。角度参数和IoU之间的关系是一个多项式函数。这种回归不一致很可能使训练收敛性和检测性能恶化。

>Moreover, in five-parameter system, parameters i.e. angle, width, height and center point have different measurement units, and show rather different relations against the Intersection over Union (IoU)

<div align="center">
    <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012170610472.png" alt="img" width=450 />
</div>

为了避免固有参数的回归不一致的问题，可采用较为流行的**Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) 八参数法**。基于八参数回归的检测直接使对象的四个点回归，因此预测是四边形的。四边形回归的关键步骤是预先对四个角点进行排序。对于顶点顺序，作者采用基于叉积的算法来获得四个顶点的序列，算法如下所示。该算法仅适用于凸四边形，此处使用顺时针顺序进行输出。向量叉积的特点就是：**AB** × **AC** > 0时，**AC**在**AB**的逆时针方向上；**AB** × **AC** = 0时，**AC**与**AB**同线；**AB** × **AC** < 0时，**AC**在**AB**的顺时针方向上。当找到最左边的点时，只有相对的点才满足CrossProduct ( s<sub>1</sub> − p<sub>1</sub> ′ , s<sub>2</sub> − p<sub>1</sub> ′ ) × CrossProduct  ( s<sub>1</sub> − p<sub>1</sub> ′ , s<sub>3</sub> − p<sub>1</sub> ′ ) < 0 ，四边形的对角线两边的边一定与对角线是一个顺时针一个逆时针。

<div align="center">
<img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20201012171620708.png" alt="img" width=450 />
</div>

<br>

## 四、思考总结

以下是自己一些不太成熟的见解和看法：

* 环形平滑标签（Circular Smooth Label , CSL），连续损失l<sub>mr</sub>，IoU-Smooth L1 Loss都是为了解决一个关键问题：回归边界问题。从更深层次角度来看这个问题，主要是由于损失的陡增引起的。CSL的做法是直接将连续的回归变为了离散的分类问题来规避回归边界的损失陡增，牺牲一定精度的方式换取了边界回归的有效解决，但是如果需要得到更高的角度分辨率则需要将回归角度划分为更多的类，反而又增加了分类的难度。SCRDet中使用的IoU-Smooth L1 Loss则是考虑到在回归边界处，预测框和真实框的较为重合，IoU接近于1，因此利用了一个平滑项log(IoU)抑制回归损失在边界处陡增，是一个较为"优雅"的损失。在RSDet中则将理想回归路线也纳入回归过程，模型在此条件下可以实现五参数法的边交换学习，八参数法的角点交换学习，考虑多种情况将损失变得连续，是一种更深层次的，具有启发性的解决思路。
* 除了基于回归loss，分类loss，能不能从IoU loss的角度来解决这个问题呢？还真有，但是这个想法如果看了一点论文谁都能想到，但是存在几个问题：（1）在旋转目标检测中，任意两个旋转框的IoU计算过程是不可导的（2）用像素来近似IoU的想法比较糙，因此需要对IoU做进一步的近似。比如[PIoU ECCV 2020](https://arxiv.org/abs/2007.09584)通过**两个核函数的乘积来近似模拟**直接累积内部重叠像素计算IoU的方法，虽然略显粗糙但可导了，整体是能正常训练的。但是，这篇文章baseline选得略低...
* 有什么可以改进的地方？从输入来看，一般遥感图片的尺度都比较大，现有基于ResNet101，ResNet152，Hourglass101作为backbone的检测网络都不可避免得存在显存消耗大的问题，可以尝试将原始图像进行8x8分块DCT变换，求解DCT系数，既保留了原始图片的低频信息同时也保留了高频信息，输入尺度也变为原始图像的1/8，并利用通道注意力机制让模型自行学习低频和高频的占比分量（类似于SE-Net），具体操作可以参见[CVPR2020](https://arxiv.org/pdf/2002.12416.pdf)一篇频域学习的文章，正在做实验看效果...

<br>

## 五、参考资料

### 5.1 相关链接

* ECCV2020｜遥感旋转目标检测方法解读：http://www.360doc.com/content/20/0708/21/32196507_923054270.shtml
* 知乎 | 遥感目标检测相关论文：https://zhuanlan.zhihu.com/p/98703562
* GITHUB | DOTA 数据集相关的代码库：https://github.com/SJTU-Thinklab-Det/DOTA-DOAI
* 上交大牛 YangXue | 遥感目标检测个人论文说明：https://www.zhihu.com/people/flyyoung-68/posts

### 5.2 参考论文

* [1] Yang X , Yan J . **Arbitrary-Oriented Object Detection with Circular Smooth Label**. *ECCV*. 2020.
* [2] Zhou X, Wang D, Krähenbühl P. **CenterNet: Objects as points**. *CVPR*. 2019.
* [3] Qian W, Yang X, Peng S, et al. **RSDet: Learning modulated loss for rotated object detection**. *CVPR*. 2019.
* [4] Yang X, Yang J, Yan J, et al. **SCRDet: Towards more robust detection for small, cluttered and rotated objects**. *CVPR*. 2019.
* [5] Xu Y, Fu M, Wang Q, et al. **Gliding Vertex: Gliding vertex on the horizontal bounding box for multi-oriented object detection**. *TPAMI*. 2020.
* [6] Zhou L, Wei H, Li H, et al. **O<sup>2</sup>-DNet: Objects detection for remote sensing images based on polar coordinates**. *arXiv:2001.02988*. 2020.
* [7] Wei H, Zhou L, Zhang Y, et al. **P-RSDet: Oriented objects as pairs of middle lines**. *arXiv:1912.10694*. 2019.
* [8] Yi J, Wu P, Liu B, et al. **BBAVector: Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors**. *WACV*. 2020.
* [9] Xu K, Qin M, Sun F, et al. **Learning in the Frequency Domain**. *CVPR*. 2020: 1740-1749.

<br>
