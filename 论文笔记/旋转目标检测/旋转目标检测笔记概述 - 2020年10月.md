# 旋转目标检测

**本次旋转目标检测的笔记是针对即将进行的火箭军AI大赛科目四做的相关调研，总结了截止2020年10月以来，在遥感图像的小目标、密集、任意方向下的旋转目标检测方法，以及当前已有方法下仍旧存在的问题**。如有纰误，敬请指正。

<div align="center">
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201010184443353.png" alt="img" width=350 />
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201010184451186.png" alt="img" width=350 />
</div>

<br>

## 一、笔记介绍

* 目前目标检测的工作集中在三个方面：**水平区域目标检测（Horizontal region object detection），任意方向目标检测（Arbitrary-oriented object detection），方位信息分类（Classification for orientation information）**。**水平区域目标检测**即为经典的目标检测方法，旨在利用常规水平框检测图像的目标，如`FasterRCNN`，`YOLO`，`SSD`是常见的**Anchor-based**的方法，而`CenterNet`，`CornerNet`则是**Anchor-free**的方法；**任意方向目标检测**则是希望对任意方向上的目标能够用一个四边形的框更准确的表示物体的范围，主流的方法可分为三种：基于旋转候选框的五参数法、基于候选框顶点偏移的八参数法和基于极坐标角度回归的五参数法。常见Baseline有`ICN`，`	ROI-Transformer`，`SCRDet`，`R3Det`等方法。最后一种**方位信息分类**的方法则是用于诸如人脸方向检测等，不在该笔记的讨论之内。
* 遥感旋转检测研究比文字晚一些，目前也有方法是从文字检测中迁移过来，但个人认为如果从旋转框角度来做的话，遥感可能会更难一些。毕竟文字是单类别，很多方法都是提出有针对性的tricks，而遥感更需要考虑不同物体的类别，直接放到遥感就不适用。遥感的旋转目标检测需要考虑更general的思路，其难点主要包括小目标 （small objects）、密集 （cluttered arrangement）、方向任意（arbitrary orientations）。

* 根据已有的旋转目标检测的代表性方法（不完全统计，更多统计的点击**第5.1节的链接[3]**），按照是否基于Anchor，单/两阶段，参数类型进行分类：

  |       Model        |   Backbone   |                          Paper Link                          | Stage  |    Anchor    |                          Parameters                          |
  | :----------------: | :----------: | :----------------------------------------------------------: | :----: | :----------: | :----------------------------------------------------------: |
  |  R<sup>3</sup>Det  |  ResNet152   |        [HRSC 2016](https://arxiv.org/abs/1908.05612)         | Single | Anchor-based |                   5-Params: Center+(w,h)+θ                   |
  |  R<sup>2</sup>CNN  |  ResNet101   |        [CVPR 2017](https://arxiv.org/abs/1706.09579)         |  Two   | Anchor-based | 5-Params: (x<sub>1</sub>,y<sub>1</sub>,x<sub>2</sub>,y<sub>2</sub>,h) |
  |       SCRDet       |  ResNet101   | [ICCV 2019](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf) |  Two   | Anchor-based |                   5-Params: Center+(w,h)+θ                   |
  |   Gliding Vertex   |  ResNet101   |  [TPAMI 2019](https://ieeexplore.ieee.org/document/9001201)  | Single | Anchor-free  | 8-Params: Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) |
  |      P-RSDet       |  ResNet101   |     [arXiv:2001.02988](https://arxiv.org/abs/2001.02988)     | Single | Anchor-free  |    5-Params: Center+*ρ*+(*θ<sub>1</sub>*,*θ<sub>2</sub>*)    |
  | O<sup>2</sup>-DNet | Hourglass104 |     [arXiv:1912.10694](https://arxiv.org/abs/1912.10694)     | Single | Anchor-free  | 8-Params: 2x(Δx<sub>1</sub>,Δx<sub>2</sub>,Δy<sub>1</sub>,Δy<sub>2</sub>) |
  |     BBAVector      |  ResNet101   |        [WACV 2020](https://arxiv.org/abs/2008.07043)         | Single | Anchor-free  |      12-Params: Center+(**t**,**r**,**l**,**b**)+(w,h)       |

<br>

## 二、预测参数的定义

本节主要介绍旋转目标检测中具有代表性和创新性的旋转框定义方法，当然并不局限于这几种，但是殊途同归，很多不同的表示方式都基于相似的思想并存在相同的问题。

### 2.1 Center+(w,h)+θ 五参数法

Center+(w,h)+θ 五参数法经典的方法有`RRPN`，`ROI-Transformer`，`R2CNN`，`R3Det`等，目前有两种主流表示方法：OpenCV法（a）和长边表示法（b）。

* OpenCV表示法（a），回归角度为-90到0度：
  * 沿**水平方向**定义参考线，位于该参考线上垂直坐标最小的顶点作为参考原点
  * **逆时针旋转**参考线，参考线接触的第一个矩形边定义为**宽度w**，而与该边垂直的另一条边定义为**高度h**
  * 旋转矩形框的中心点坐标为（x，y），旋转角度为 θ， θ∈[-90, 0]
* 长边表示法（b），回归角度为-90度到90度：
  * 沿**水平方向**定义参考线，选取矩形框的一个**最长边h**，并以该边水平方向最小的顶点作为参考原点
  * 角度 θ 为**长边h**与水平方向的**x轴**所形成的夹角，因此角度的范围是 θ∈[-90, 90]

<div align="center">
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201010084120280.png" alt="img" width=450 />
</div>


虽然这两种表示方式在实际遥感和文字的旋转目标检测应用中取得了比较出色的成果，但是由于大长宽比的目标、训练的损失对于角度的变化和边交换是非常敏感的，因此这两种表示方法都存在**回归难度大**、**损失不连续**、**水平框预测差**，**度量单位（量纲）不一致**的问题。

### 2.2 Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) 八参数法

Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>) 八参数法是一种更为巧妙的四边形定义法，相比五参数法需要在训练前对四边形框的四个点按照一定规则进行排序。首先了解一下为什么要排序：如果一个四边形的ground-truth是（x1,y1,x2,y2,x3,y3,x4,y4）并且所有的ground-truth并不是按一定规则顺序标注的，那么检测器有可能给出的预测结果是（x2,y2,x3,y3,x4,y4,x1,y1）。其实这两个是框是完全重合的，但是网络训练算损失的时候并不知道，它会按对应位置计算损失，此时的损失值并不为0甚至很大。而Center+(w,h)+(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,a<sub>4</sub>)八参数法则考虑到了顺序标签点（sequential label points）的问题。

<div align="center">
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201011232343294.png" alt="img" width=500 />
</div>

基于上述问题，[Gliding Vertex](https://ieeexplore.ieee.org/document/9001201)通过改变框的表示方式避免了排序的麻烦：

> By limiting the offset on the corresponding side of horizontal bounding box, we may facilitate offset learning and also avoid the confusion for sequential label points in directly regressing the four vertices of oriented objects. 

先检测水平框，这个是没有序列问题的，然后**学习水平框四个角点的偏移量**来达到四边形检测的目的，其实这里的（偏移量，对应的水平框的点）配对就有排序的意思了。相比较而言，[RSDet](https://arxiv.org/pdf/1911.08299.pdf)则是对角点进行了排序（对ground-truth的排序并不耗时，对检测器影响几乎忽略不计），并给出了一种角点的排序算法：其主要步骤是先确定最左边的点（如果水平矩形的话有两个点满足取上面的那个）。然后通过[叉乘（向量积）](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%90%91%E9%87%8F%E7%A7%AF/4601007%3Ffr%3Daladdin)找到对角点，也就是第三个点。最后利用这两个点构成的向量以及叉乘方法，根据顺序的要求（逆时针或者顺时针）找点其他两个点。

八参数法一定程度上解决了五参数法中度量单位不一致的弊端，而且相比于对矩形框进行旋转并预测的五参数法，八参数法能够表征更为复杂的四边形框，对具有仿射变换性质的图像（如文字检测）中，能直接避免五参数法后续需要对检测框进行微调的问题。但是依然存在**回归难度大**、**损失不连续**、**水平框预测差**的问题。

### 2.3 Center+*ρ*+(*θ<sub>1</sub>*,*θ<sub>2</sub>*) 极坐标五参数法

上述五参数法和八参数法都有一个共同的特征：**基于笛卡尔坐标系**对物体对象进行建模，这种直角坐标系在呈现水平边界框方面具有优势，但是在旋转目标检测上却显得尤为复杂；极坐标在旋转和方向相关的问题上具有优势，极坐标的合理使用可以简化对象建模并降低模型的复杂性。[P-RSDet](https://arxiv.org/pdf/2001.02988.pdf)极坐标下的目标检测器, 提出了一种新颖的以极坐标为模型的**Anchor-free**检测器来检测遥感图像的对象，这使得定向输出形式的获取与水平形式的获取一样简单。这一称为极坐标遥感物体检测器（P-RSDet）的模型以每个物体的中心点为极点，以水平正方向为极轴来建立极坐标系。可以将一个物体的检测视为水平和定向包围盒的一个极半径和两个极角的预测。

<div align="center">
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201012095919099.png" alt="img" width=400 />
</div>

* 在极坐标中，一个点可以用（ρ;θ）表示，其中r是极半径，q是极角。 当以中心点为极坐标的极点时，定向边界框可以用（x; y;ρ）表示，如（d）所示。对于一个边界框，根据原始注释（x1，y1），（x2，y2），（x3，y3），（x4，y4），令在极坐标系下的极点为 Center（x<sub>p</sub>，y<sub>p</sub>）。 由于人工标注的误差，四个角与极点之间的距离不一定相等，因此将四个半径的平均值作为极半径的目标回归值。P-RSDet的其余参数定义方法为：
  * 以每个物体的中心点为极点，以水平正方向为极轴来建立极坐标系，并将极角以弧度表示
  * 在这个坐标系中，四个角可以依次表示为（ρ1，θ1），（ρ2，θ2），（ρ3，θ3），（ρ4，θ4）
  * 根据矩形的属性，我们可以得到以下内容关系：ρ1 = ρ2 = ρ3 = ρ4，θ3 = θ1 + π; θ4 = θ2 + π
  * 因此，令ρ=ρ1=ρ2=ρ3=ρ4，仅需要五个变量：Center，ρ，θ1和θ2来表示极坐标中对象的边界框

### 2.4 Center+(**t**,**r**,**l**,**b**)+(w,h) 边缘感知向量

为了得到带方向的包围框，一个很自然的想法就是回归出宽，高，和角度θ，我们把这个baseline称为Center+wh+θ的五参数法，如图（a）。这个方法有几个缺点：（1）小的角度的变化对于损失来说可能是微不足道的，但是对于IOU却会产生较大的影响；（2）Oriented bounding box 中的w，h是在各自独立的旋转坐标系统中定义的，角度θ是相对于y轴的，这样，网络很难去联合训练所有的物体；（3）w,h和θ的度量单位不一致，和IoU之间表现出的是不同的关系。

<div align="center">
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201012111648393.png" alt="img" width=400 />
</div>

所以，[BBAVector](https://arxiv.org/abs/2008.07043)提出了一种描述OBB的方法，叫做box boundary-aware vectors，包括4个向量，**t，r，b，l**，共12个预测参数。在此设计中，这4个向量分布在**笛卡尔坐标系的4个象限**中，所有的旋转物体都共用一个坐标系，这样可以高效的利用共同的信息，并提升模型的泛化能力。模型中设计了4个向量，而不是2个，为的是当某些局部特征不是很明确的时候可以得到更多的边界交互信息。但是在实际做的过程中，在一些极端情况下，向量和象限的边界非常的靠近，如图（c），这样区分向量的类型就比较困难，即**水平框预测差**，这是旋转框预测的常见问题，参见3.3节。

## 三、存在的问题

### 3.1 回归难度大

回归难度大体现在，由角度的周期性（**PoA**）和边的可交换性（**EoE**）导致**理想的回归路线的损失很大**（也就是在实际训练过程中，模型走了弯路）。YangXue在[SCRDet ICCV2019](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf)以及[CSL ECCV 2020](http://arxiv.org/abs/2003.05597)中均有提出：

>loss of this situation is very large due to the periodicity of angular (PoA) and exchangeability of edges (EoE)  

<div align="center">
    <img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201012145654751.png" alt="img" height=350 />
</div>


* 基于90°回归的方法，如上图（a）所示。蓝色框、红色框和绿色框分别是参考候选框，预测框和真实框，**特别注意这几个框的长h和宽w所在的位置！！**最理想的角度回归路线是由参考候选框（蓝色）逆时针旋转得到预测框（红色），预测框（红色）和真实框（绿色）便可以基本重合。但是模型并不会将hw进行交换（矩形框存在边的可交换性**EoE**），导致理想的回归的损失非常大。因此实际训练过程中，模型会迫使参考候选框顺时针旋转到灰色框的等效位置（角度的周期性**PoA**）并缩放长宽，从而加大了回归的难度。（按照我个人的理解：角度的周期性PoA应该描述为**角度的等效性**更加合适）
* 基于180°回归的方法，即长边表示法，如上图（b）所示，该方法避免了矩形框的边的可交换性的问题，但是类似地，该方法也存在由**PoA**引起的损损剧增的问题，即该模型最终将参考候选框顺时针旋转一个大角度，以获得最终的预测边界框。**（注意这里图有错误：蓝色候选框，绿色真实框，红色预测框的角度应该分别是π/2，5π/8，3π/8**）
* 基于角点偏移的方法，如图（c）所示。 通过进一步分析：由于角点的提前排序，八参数回归方法中仍然存在边界不连续性问题。考虑边界情况下八参数回归的情况，理想的从蓝色参考框到绿色真实框的回归过程应如虚线箭头所示：（<span style="color:blue;">a</span>-><span style="color:green;">b</span>）（<span style="color:blue;">b</span>-><span style="color:green;">c</span>）（<span style="color:blue;">c</span>-><span style="color:green;">d</span>）（<span style="color:blue;">d</span>-><span style="color:green;">a</span>），但是实际回归过程可能为：（<span style="color:blue;">a</span>-><span style="color:green;">a</span>）（<span style="color:blue;">b</span>-><span style="color:green;">b</span>）（<span style="color:blue;">c</span>-><span style="color:green;">c</span>）（<span style="color:blue;">d</span>-><span style="color:green;">d</span>）。 相比之下，蓝色到红色边界框的实际和理想回归是一致的，但是这种实际的回归过程却会导致损失变大。

综上YangXue在[CSL](https://arxiv.org/abs/2003.05597)中得出这样的结论：**理想的预测结果超出了我们所定义的范围，导致出现边界问题，即产生一个较大的损失值。**

>We argue that the root cause of boundary problems based on regression methods is that the ideal predictions are beyond the defined range. 



在2.1节中定义的五参数法，使用旋转边界框进行检测，角度预测的准确性至关重要，较小的角度偏差即可导致IoU下降，从而导致目标检测不准确，而且如果。而



RRPN应该是第一个基于RPN架构引入旋转候选框实现任意方向的场景文本检测。基于旋转的anchor得到旋转ROI，然后提取相应特征。

### 3.2 损失不连续

### 3.3 水平框预测差

### 3.4 度量单位（量纲）不一致





我认为他的做法是牺牲了一定的预测精度（通过水平框来近似）来逃避了边界预测不好的问题，但是确实也是有一定效果的。另外，边界问题是否真的是它认为的那样也是模糊的？







* Center+(w,h)+θ五参数法



>Moreover, in five-parameter system, parameters i.e. angle, width, height and center point have different measurement units, and show rather different relations against the Intersection over Union (IoU)   





<div align="center">
<img src="assets/旋转目标检测笔记概述 - 2020年10月/image-20201010090605328.png" alt="img" width=500 />
</div>



<br>

## 四、个人思考



<br>

## 五、参考资料

#### 5.1 相关链接

* ECCV2020｜遥感旋转目标检测方法解读：http://www.360doc.com/content/20/0708/21/32196507_923054270.shtml
* 知乎 | 遥感目标检测相关论文：https://zhuanlan.zhihu.com/p/98703562
* GITHUB | DOTA 数据集相关的代码库：https://github.com/SJTU-Thinklab-Det/DOTA-DOAI
* 上交大牛 YangXue | 遥感目标检测个人论文说明：https://www.zhihu.com/people/flyyoung-68/posts

#### 5.2 参考论文

* [1] Yang X , Yan J . **Arbitrary-Oriented Object Detection with Circular Smooth Label**. *ECCV*. 2020.
* [2] Zhou X, Wang D, Krähenbühl P. **CenterNet: Objects as points**. *CVPR*. 2019.
* [3] Qian W, Yang X, Peng S, et al. **RSDet: Learning modulated loss for rotated object detection**. *CVPR*. 2019.
* [4] Yang X, Yang J, Yan J, et al. **SCRDet: Towards more robust detection for small, cluttered and rotated objects**. *CVPR*. 2019.
* [5] Xu Y, Fu M, Wang Q, et al. **Gliding Vertex: Gliding vertex on the horizontal bounding box for multi-oriented object detection**. *TPAMI*. 2020.
* [6] Zhou L, Wei H, Li H, et al. **O<sup>2</sup>-DNet: Objects detection for remote sensing images based on polar coordinates**. *arXiv:2001.02988*. 2020.
* [7] Wei H, Zhou L, Zhang Y, et al. **P-RSDet: Oriented objects as pairs of middle lines**. *arXiv:1912.10694*. 2019.
* [8] Yi J, Wu P, Liu B, et al. **BBAVector: Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors**. *WACV*. 2020.

<br>